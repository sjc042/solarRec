{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script converts format of object detection annotation file from YOLO square bounding box to YOLO mask/polygon.\n",
    "#### The conversion is done via Segment Anything Model (SAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Append path to Segment Anything Model (SAM)\n",
    "sys.path.append(\"..\")  # NOTE: Adjust this path based on your SAM installation\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "# from segment_anything.utils.transforms import ResizeLongestSide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Global Hyperparameters\n",
    "EPS = 0.01     # larger value results fewer vertices for a polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Uitility Fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_valid_annotation(input_dir_path, output_dir_path):\n",
    "    non_empty_files = []\n",
    "    for filename in os.listdir(input_dir_path):\n",
    "        file_path = os.path.join(input_dir_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            if os.path.getsize(file_path) > 0:\n",
    "                non_empty_files.append(file_path)\n",
    "            else:\n",
    "                # copy txt file to output_dir_path\n",
    "                shutil.copy(file_path, os.path.join(output_dir_path, filename))\n",
    "    return non_empty_files\n",
    "\n",
    "def query_bboxes(image_height, image_width, label_path):\n",
    "    converted_annotations = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            obj_class, x_center, y_center, width, height = map(float, line.split())\n",
    "            x_center, y_center = x_center * image_width, y_center * image_height\n",
    "            width, height = width * image_width, height * image_height\n",
    "            top_left_x = x_center - (width / 2)\n",
    "            top_left_y = y_center - (height / 2)\n",
    "            bottom_right_x = x_center + (width / 2)\n",
    "            bottom_right_y = y_center + (height / 2)\n",
    "            converted_annotations.append([top_left_x, top_left_y, bottom_right_x, bottom_right_y])\n",
    "    return converted_annotations\n",
    "\n",
    "def prepare_image(image_path, transform, device):\n",
    "    # Prepare Images for SAM\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = transform.apply_image(image)\n",
    "    image = torch.as_tensor(image, device=device)\n",
    "    return image.permute(2, 0, 1).contiguous()\n",
    "\n",
    "def mask_to_polygons(mask, image_h, image_w):\n",
    "    \"\"\"\n",
    "    Converts a mask (numpy array) of multiple object instances to a list of polygons\n",
    "        with normalized vertex values.\n",
    "    \"\"\"\n",
    "    # Perform erosion followed by dilation (opening)\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    mask = cv2.erode(mask, kernel, iterations = 1)\n",
    "    mask = cv2.dilate(mask, kernel, iterations = 1)\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    # for our application, there is just one contour for one object\n",
    "    for i, contour in enumerate(contours):\n",
    "        # Approximate contour to polygon\n",
    "        epsilon = EPS * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True).astype(float)\n",
    "\n",
    "        # Flatten and convert to list of points\n",
    "        polygon = approx.flatten().reshape(-1, 2)\n",
    "\n",
    "        # Normalize the coordinate values with respect to image width and height\n",
    "        polygon[:, 0] /= image_w\n",
    "        polygon[:, 1] /= image_h\n",
    "        \n",
    "        polygon = polygon.tolist()\n",
    "        polygons.append(polygon)\n",
    "    return polygons\n",
    "\n",
    "def save_polygons(class_idx, polygons, filename):\n",
    "    \"\"\"\n",
    "    Saves polygons to a text file.\n",
    "\n",
    "    class_idx (str): class index of all polygons\n",
    "    polygons (list): shape is N x M x 2, N is number of polygons (objects), M is number of vertices, \n",
    "                        and vertex coordinates in [x, y]\n",
    "    filename (str): text file path to save\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for polygon in polygons:\n",
    "            # Format polygon points for saving\n",
    "            # Assuming the format: 'x1,y1 x2,y2 x3,y3,...'\n",
    "            line = class_idx + ' ' + ' '.join([f'{coord[0]} {coord[1]}' for coord in polygon])\n",
    "            file.write(f'{line}\\n')\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n",
    "\n",
    "def visualize_mask_image(image, masks, boxes_tensor):\n",
    "    # Setup a figure with 2 subplots (1 row, 2 columns)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    # First subplot with masks and boxes\n",
    "    axs[0].imshow(image)\n",
    "    for mask in masks:\n",
    "        # print(np.unique(mask.cpu().numpy(), return_counts=True))\n",
    "        show_mask(mask.cpu().numpy(), axs[0], random_color=True)  # Pass the appropriate Axes object\n",
    "    for box in boxes_tensor:\n",
    "        show_box(box.cpu().numpy(), axs[0])  # Pass the appropriate Axes object\n",
    "    axs[0].axis('off')  # Turn off axis for the first subplot\n",
    "\n",
    "    # Second subplot with only the image\n",
    "    axs[1].imshow(image)\n",
    "    axs[1].axis('on')  # Keep the axis on for the second subplot\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout to make sure there's no overlap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Segment Anython Model (SAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = \"/home/psc/Desktop/segment-anything/assets/weights/sam_vit_h_4b8939.pth\"  # NOTE: change this to custom path to SAM weight\n",
    "model_type = \"vit_h\"\n",
    "assert torch.cuda.is_available()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Main Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645/645 [07:41<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Specify data root path\n",
    "data_root = '/home/psc/Desktop/PSC/Custom_Dataset/custom-dataset-4120.v2i.yolov8/train'     # NOTE: change this to your dataset path\n",
    "image_dir = os.path.join(data_root, 'images')\n",
    "label_dir = os.path.join(data_root, 'labels')\n",
    "# Specify polygon label save directory\n",
    "polygon_label_dir = os.path.join(data_root, 'labels-polygon')\n",
    "os.makedirs(polygon_label_dir, exist_ok=True)\n",
    "\n",
    "non_empty_files = query_valid_annotation(label_dir, polygon_label_dir)\n",
    "\n",
    "# Convert the image / label one at a time\n",
    "for i, label_path in enumerate(tqdm(non_empty_files, total=len(non_empty_files))):\n",
    "\n",
    "    # if i != 6: continue\n",
    "    polygon_label_path = os.path.join(polygon_label_dir, os.path.basename(label_path))\n",
    "    # print(polygon_label_path)\n",
    "\n",
    "    image_name = os.path.basename(label_path).replace('.txt', '.jpg')\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    assert os.path.exists(image_path), f\"{image_path} not found.\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_h, image_w = image.shape[:2]\n",
    "    boxes = query_bboxes(image_h, image_w, label_path)\n",
    "    # Convert boxes to tensor and transform to current image\n",
    "    boxes_tensor = torch.tensor(boxes, dtype=torch.float, device=device)\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(boxes_tensor, image.shape[:2])\n",
    "\n",
    "    # # Preparing image for SAM\n",
    "    # prepared_image = prepare_image(image_path, ResizeLongestSide(sam.image_encoder.img_size), device)\n",
    "    \n",
    "    # Process the image to produce an image embedding for mask predicton\n",
    "    predictor.set_image(image)\n",
    "    \n",
    "    # Make mask prediciton\n",
    "    masks, _, _ = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    # Optionally Visualize the mask with original image\n",
    "    # visualize_mask_image(image, masks, boxes_tensor)\n",
    "\n",
    "    \n",
    "    mask = masks[0]\n",
    "    \n",
    "    # Assume each mask only contains one instance\n",
    "    # So here we perform bitwise-or for all masks of individual objects of the same class\n",
    "    for m in masks[1:]:\n",
    "        mask += m\n",
    "    \n",
    "    mask = torch.squeeze(mask, 0).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    # Convert mask image to polygons\n",
    "    polygons = mask_to_polygons(mask, image_h, image_w)\n",
    "    \n",
    "    save_polygons('0', polygons, polygon_label_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
